# Track-MDP: Reinforcement Learning for Target Tracking with Controlled Sensing

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Ray RLlib](https://img.shields.io/badge/Ray-RLlib-orange.svg)](https://docs.ray.io/en/latest/rllib/index.html)

Implementation of the paper: "Track-MDP: Reinforcement Learning for Target Tracking with Controlled Sensing" (**ICASSP 2025**)

Adarsh M. Subramaniam, **Argyrios Gerogiannis**, James Z. Hare, Venugopal Veeravalli 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)

## ğŸŒŸ Features

- **Grid-based Object Tracking**: Stochastic object movement in configurable NxN grid environments
- **Sensor Network Optimization**: Intelligent sensor activation to minimize energy consumption while maximizing tracking accuracy
- **Ray RLlib Integration**: Built on Ray RLlib for scalable distributed training
- **Real-time Visualization**: Interactive pygame-based visualization of tracking performance
- **Comparative Evaluation**: Built-in QMDP baseline comparison for performance analysis
- **Modular Architecture**: Clean separation of environment, training, evaluation, and visualization components
- **Flexible Configuration**: Easy parameter tuning for different scenarios and environments


## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (optional, for faster training)

### Install from Source

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/Track-MDP-final.git
   cd Track-MDP-final
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv track_mdp_env
   source track_mdp_env/bin/activate  # On Windows: track_mdp_env\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install the package:**
   ```bash
   pip install -e .
   ```

## ğŸƒ Quick Start

### Training

Train a tracking agent with default parameters:

```python
from src.training.trainer import train

# Train wihout visualization
train(visualization_mode='none')
```

### Visualization and Evaluation

Visualize and evaluate a trained policy:

```bash
python visualize.py --checkpoint ./agent_run194_a2c --episodes 100 --fps 10
```

## ğŸ“ Project Structure

```
Track-MDP-final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                    # Core environment and gym wrapper
â”‚   â”‚   â”œâ”€â”€ environment.py       # Grid environment implementation
â”‚   â”‚   â””â”€â”€ gym_wrapper.py       # Gymnasium environment wrapper
â”‚   â”œâ”€â”€ training/                # Training modules
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Main training script
â”‚   â”‚   â””â”€â”€ monitor.py           # Training monitoring and visualization
â”‚   â”œâ”€â”€ evaluation/              # Evaluation and testing
â”‚   â”‚   â”œâ”€â”€ evaluator.py         # Policy evaluation functions
â”‚   â”‚   â””â”€â”€ visualizer.py        # Interactive visualization
â”‚   â”œâ”€â”€ visualization/           # Rendering and display
â”‚   â”‚   â””â”€â”€ renderer.py          # Pygame-based visualization
â”‚   â””â”€â”€ utils/                   # Utility functions
â”œâ”€â”€ assets/                      # Image assets for visualization
â”‚   â”œâ”€â”€ robot4.png               # Robot/object sprite
â”‚   â”œâ”€â”€ antenna_on.png           # Active sensor sprite
â”‚   â””â”€â”€ antenna_off.png          # Inactive sensor sprite
â”œâ”€â”€ examples/                    # Usage examples
â”œâ”€â”€ visualize.py                 # Standalone visualization script
â”œâ”€â”€ comparative_evaluation.py    # QMDP vs Track-MDP comparison
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup.py                     # Package installation
â””â”€â”€ README.md                    # This file
```


## ğŸ“Š Performance Evaluation

### Metrics

The framework provides comprehensive performance metrics:

- **Tracking Accuracy**: Proportion of time steps where object is successfully detected
- **Sensor Efficiency**: Average number of sensors activated per time step
- **Average Reward**: Average cumulative reward including tracking success and sensor costs
- **Cost per Detection**: Average sensor cost per successful object detection

### Baseline Comparison

Use the built-in QMDP baseline for performance comparison:

```bash
python comparative_evaluation.py
```

This will output a detailed comparison showing:
- Tracking accuracy improvements
- Sensor efficiency gains  
- Reward optimization



## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/your-feature`
3. **Make your changes** and add tests
4. **Run the test suite**: `python -m pytest`
5. **Submit a pull request**

### Development Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/Track-MDP-final.git
cd Track-MDP-final

# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run tests
python -m pytest tests/
```

### Code Style

We use:
- **Black** for code formatting
- **Flake8** for linting  
- **MyPy** for type checking
- **Pytest** for testing

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built on [Ray RLlib](https://docs.ray.io/en/latest/rllib/) for distributed reinforcement learning
- Visualization powered by [Pygame](https://www.pygame.org/)


## ğŸ”— Related Projects

- [Ray RLlib](https://github.com/ray-project/ray)
- [OpenAI Gym](https://github.com/openai/gym)
- [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3)

---

**Happy Tracking!** ğŸ¯
